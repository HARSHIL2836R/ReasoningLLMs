{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90ac20b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "from together import Together\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e87cfd",
   "metadata": {},
   "source": [
    "### Self Consistency Implementation with majority vote decision and comparison with greedy decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b01cb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize Together client\n",
    "# os.environ[\"TOGETHER_API_KEY\"] =\"3116fd3668302432d738187aa87a1ef3d5f89559b73d736b02f31da487baebd0\"\n",
    "load_dotenv()\n",
    "client = Together()\n",
    "\n",
    "# Set Together AI API Key\n",
    "# os.environ[\"TOGETHERAI_API_KEY\"] = os.getenv(\"TOGETHER_API_KEY\")\n",
    "\n",
    "total_questions = 100\n",
    "random.seed(0)\n",
    "\n",
    "# Load a random subset of 3 questions from the validation set\n",
    "dataset = load_dataset(\"commonsense_qa\", split=\"validation\")\n",
    "sampled_dataset = random.sample(list(dataset), total_questions)\n",
    "\n",
    "def format_prompt(question, choices):\n",
    "    choice_str = \"\\n\".join([f\"{label}. {text}\" for label, text in zip(choices['label'], choices['text'])])\n",
    "    return f\"Question: {question}\\nChoices:\\n{choice_str}\\nAnswer with the correct letter\"\n",
    "\n",
    "def get_response(prompt, temperature=0.0):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def sc_most_common(question, choices, n_samples=3):\n",
    "    prompt = format_prompt(question, choices)\n",
    "    answers = [get_response(prompt, temperature=0.7) for _ in range(n_samples)]\n",
    "\n",
    "    return Counter(answers).most_common(1)[0][0],answers\n",
    "\n",
    "# Evaluate self-consistency\n",
    "self_consistency_accuracy = 0\n",
    "greedy_accuracy = 0\n",
    "difference = 0\n",
    "better = 0\n",
    "worse = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fb10630",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = \"mistral_7b_instruct_output.csv\"\n",
    "if os.path.exists(csv_file_path):\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "else:\n",
    "    df = pd.DataFrame(columns=['Question', 'Choices', 'Answer Match Greedy', 'Answer Match Self-consistency', 'Correct Answer', 'Greedy Answer', 'Self Answer','Approaches Agree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0293c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_option(answer):\n",
    "    \"\"\"Extract the option (e.g., 'A', 'B', etc.) from the answer text.\"\"\"\n",
    "    for option in ['A', 'B', 'C', 'D', 'E']:\n",
    "        if answer.startswith(option) or f\"{option}.\" in answer or f\"{option} \" in answer:\n",
    "            return option\n",
    "    return None  # Return None if no valid option is found\n",
    "\n",
    "def run_tests(sampled_dataset,self_consistent_answer):\n",
    "    for idx, item in enumerate(sampled_dataset, 1):\n",
    "        if item[\"question\"] in df['Question'].values:\n",
    "            print(f\"Skipping question {idx} as it is already in the dataframe.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n--- Question {idx} ---\")\n",
    "        df.loc[idx, 'Question'] = item[\"question\"]\n",
    "        choice_str = \" \".join([f\"{label}. {text}\" for label, text in zip(item[\"choices\"]['label'], item[\"choices\"]['text'])])\n",
    "        df.loc[idx, 'Choices'] = choice_str\n",
    "        greedy_answer = get_response(format_prompt(item[\"question\"], item[\"choices\"]), temperature=0.0)\n",
    "        df.loc[idx, 'Greedy Answer'] = greedy_answer\n",
    "\n",
    "        n = 6\n",
    "        prediction, answers = self_consistent_answer(item[\"question\"], item[\"choices\"], n_samples=n)\n",
    "        df.loc[idx, 'Self Answer'] = prediction\n",
    "        correct_answer = item[\"answerKey\"] \n",
    "        index = item['choices']['label'].index(item['answerKey'])\n",
    "        correct_answer = correct_answer + \". \" + item['choices']['text'][index]    \n",
    "\n",
    "        df.loc[idx, 'Correct Answer'] = correct_answer\n",
    "\n",
    "        # Extract options from greedy_answer and prediction\n",
    "        greedy_option = extract_option(greedy_answer)\n",
    "        self_option = extract_option(prediction)\n",
    "\n",
    "        if correct_answer[0] == greedy_option:\n",
    "            df.loc[idx, 'Answer Match Greedy'] = True\n",
    "        else:\n",
    "            df.loc[idx, 'Answer Match Greedy'] = False\n",
    "        \n",
    "        if correct_answer[0] == self_option:\n",
    "            df.loc[idx, 'Answer Match Self-consistency'] = True\n",
    "        else :\n",
    "            df.loc[idx, 'Answer Match Self-consistency'] = False\n",
    "        \n",
    "        if greedy_option != self_option:\n",
    "            df.loc[idx, \"Approaches Agree\"] = False\n",
    "        else:\n",
    "            df.loc[idx, 'Approaches Agree'] = True\n",
    "\n",
    "        df.to_csv(\"mistral_7b_instruct_output.csv\")\n",
    "\n",
    "    print(\"--- Evaluation ---\")\n",
    "    greedy_accuracy = df['Answer Match Greedy'].mean() * 100  # Percentage of correct greedy answers\n",
    "    self_consistency_accuracy = df['Answer Match Self-consistency'].mean() * 100  # Percentage of correct self-consistent answers\n",
    "\n",
    "    # Calculate differences\n",
    "    total_questions = len(df)\n",
    "    disagreements = df[df['Approaches Agree'] == False].shape[0]  # Count of disagreements\n",
    "    better_self = df[(df['Approaches Agree'] == False) & (df['Answer Match Self-consistency'] == True)].shape[0]\n",
    "    worse_self = df[(df['Approaches Agree'] == False) & (df['Answer Match Greedy'] == True)].shape[0]\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Total Questions: {total_questions}\")\n",
    "    print(f\"Greedy Accuracy: {greedy_accuracy:.2f}%\")\n",
    "    print(f\"Self-Consistency Accuracy: {self_consistency_accuracy:.2f}%\")\n",
    "    print(f\"Disagreements: {disagreements}\")\n",
    "    print(f\"Better Self-Consistency: {better_self}\")\n",
    "    print(f\"Worse Self-Consistency: {worse_self}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e00c0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 1 ---\n",
      "\n",
      "--- Question 2 ---\n",
      "\n",
      "--- Question 3 ---\n",
      "\n",
      "--- Question 4 ---\n",
      "\n",
      "--- Question 5 ---\n",
      "\n",
      "--- Question 6 ---\n",
      "\n",
      "--- Question 7 ---\n",
      "\n",
      "--- Question 8 ---\n",
      "\n",
      "--- Question 9 ---\n",
      "\n",
      "--- Question 10 ---\n",
      "\n",
      "--- Question 11 ---\n",
      "\n",
      "--- Question 12 ---\n",
      "\n",
      "--- Question 13 ---\n",
      "\n",
      "--- Question 14 ---\n",
      "\n",
      "--- Question 15 ---\n",
      "\n",
      "--- Question 16 ---\n",
      "\n",
      "--- Question 17 ---\n",
      "\n",
      "--- Question 18 ---\n",
      "\n",
      "--- Question 19 ---\n",
      "\n",
      "--- Question 20 ---\n",
      "\n",
      "--- Question 21 ---\n",
      "\n",
      "--- Question 22 ---\n",
      "\n",
      "--- Question 23 ---\n",
      "\n",
      "--- Question 24 ---\n",
      "\n",
      "--- Question 25 ---\n",
      "\n",
      "--- Question 26 ---\n",
      "\n",
      "--- Question 27 ---\n",
      "\n",
      "--- Question 28 ---\n",
      "\n",
      "--- Question 29 ---\n",
      "\n",
      "--- Question 30 ---\n",
      "\n",
      "--- Question 31 ---\n",
      "\n",
      "--- Question 32 ---\n",
      "\n",
      "--- Question 33 ---\n",
      "\n",
      "--- Question 34 ---\n",
      "\n",
      "--- Question 35 ---\n",
      "\n",
      "--- Question 36 ---\n",
      "\n",
      "--- Question 37 ---\n",
      "\n",
      "--- Question 38 ---\n",
      "\n",
      "--- Question 39 ---\n",
      "\n",
      "--- Question 40 ---\n",
      "\n",
      "--- Question 41 ---\n",
      "\n",
      "--- Question 42 ---\n",
      "\n",
      "--- Question 43 ---\n",
      "\n",
      "--- Question 44 ---\n",
      "\n",
      "--- Question 45 ---\n",
      "\n",
      "--- Question 46 ---\n",
      "\n",
      "--- Question 47 ---\n",
      "\n",
      "--- Question 48 ---\n",
      "\n",
      "--- Question 49 ---\n",
      "\n",
      "--- Question 50 ---\n",
      "\n",
      "--- Question 51 ---\n",
      "\n",
      "--- Question 52 ---\n",
      "\n",
      "--- Question 53 ---\n",
      "\n",
      "--- Question 54 ---\n",
      "\n",
      "--- Question 55 ---\n",
      "\n",
      "--- Question 56 ---\n",
      "\n",
      "--- Question 57 ---\n",
      "\n",
      "--- Question 58 ---\n",
      "\n",
      "--- Question 59 ---\n",
      "\n",
      "--- Question 60 ---\n",
      "\n",
      "--- Question 61 ---\n",
      "\n",
      "--- Question 62 ---\n",
      "\n",
      "--- Question 63 ---\n",
      "\n",
      "--- Question 64 ---\n",
      "\n",
      "--- Question 65 ---\n",
      "\n",
      "--- Question 66 ---\n",
      "\n",
      "--- Question 67 ---\n",
      "\n",
      "--- Question 68 ---\n",
      "\n",
      "--- Question 69 ---\n",
      "\n",
      "--- Question 70 ---\n",
      "\n",
      "--- Question 71 ---\n",
      "\n",
      "--- Question 72 ---\n",
      "\n",
      "--- Question 73 ---\n",
      "\n",
      "--- Question 74 ---\n",
      "\n",
      "--- Question 75 ---\n",
      "\n",
      "--- Question 76 ---\n",
      "\n",
      "--- Question 77 ---\n",
      "\n",
      "--- Question 78 ---\n",
      "\n",
      "--- Question 79 ---\n",
      "\n",
      "--- Question 80 ---\n",
      "\n",
      "--- Question 81 ---\n",
      "\n",
      "--- Question 82 ---\n",
      "\n",
      "--- Question 83 ---\n",
      "\n",
      "--- Question 84 ---\n",
      "\n",
      "--- Question 85 ---\n",
      "\n",
      "--- Question 86 ---\n",
      "\n",
      "--- Question 87 ---\n",
      "\n",
      "--- Question 88 ---\n",
      "\n",
      "--- Question 89 ---\n",
      "\n",
      "--- Question 90 ---\n",
      "\n",
      "--- Question 91 ---\n",
      "\n",
      "--- Question 92 ---\n",
      "\n",
      "--- Question 93 ---\n",
      "\n",
      "--- Question 94 ---\n",
      "\n",
      "--- Question 95 ---\n",
      "\n",
      "--- Question 96 ---\n",
      "\n",
      "--- Question 97 ---\n",
      "\n",
      "--- Question 98 ---\n",
      "\n",
      "--- Question 99 ---\n",
      "\n",
      "--- Question 100 ---\n",
      "--- Evaluation ---\n",
      "Total Questions: 100\n",
      "Greedy Accuracy: 71.00%\n",
      "Self-Consistency Accuracy: 70.00%\n",
      "Disagreements: 7\n",
      "Better Self-Consistency: 2\n",
      "Worse Self-Consistency: 3\n"
     ]
    }
   ],
   "source": [
    "def cluster_and_majority_vote(answers, threshold=0.7):\n",
    "    \"\"\"\n",
    "    Cluster answers based on cosine similarity and apply majority vote within clusters.\n",
    "    \n",
    "    Args:\n",
    "        answers (list): List of answers generated by the model.\n",
    "        threshold (float): Cosine similarity threshold to form clusters.\n",
    "        \n",
    "    Returns:\n",
    "        str: The most common answer from the largest cluster.\n",
    "    \"\"\"\n",
    "    # Convert answers to TF-IDF vectors\n",
    "    vectorizer = TfidfVectorizer().fit_transform(answers)\n",
    "    similarity_matrix = cosine_similarity(vectorizer)\n",
    "\n",
    "    # Clustering based on similarity threshold\n",
    "    clusters = []\n",
    "    visited = set()\n",
    "    for i in range(len(answers)):\n",
    "        if i in visited:\n",
    "            continue\n",
    "        cluster = [i]\n",
    "        visited.add(i)\n",
    "        for j in range(len(answers)):\n",
    "            if j not in visited and similarity_matrix[i][j] >= threshold:\n",
    "                cluster.append(j)\n",
    "                visited.add(j)\n",
    "        clusters.append(cluster)\n",
    "\n",
    "    # Find the largest cluster and apply majority vote\n",
    "    largest_cluster = max(clusters, key=len)\n",
    "    clustered_answers = [answers[i] for i in largest_cluster]\n",
    "    most_common_answer = Counter(clustered_answers).most_common(1)[0][0]\n",
    "\n",
    "    return most_common_answer\n",
    "\n",
    "# Update the self-consistency function to use clustering\n",
    "def sc_with_clustering(question, choices, n_samples=3, threshold=0.7):\n",
    "    \"\"\"\n",
    "    Self-consistency with clustering based on cosine similarity.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The question text.\n",
    "        choices (dict): The answer choices.\n",
    "        n_samples (int): Number of samples to generate.\n",
    "        threshold (float): Cosine similarity threshold for clustering.\n",
    "        \n",
    "    Returns:\n",
    "        str: The final answer after clustering and majority vote.\n",
    "    \"\"\"\n",
    "    prompt = format_prompt(question, choices)\n",
    "    answers = [get_response(prompt, temperature=0.7) for _ in range(n_samples)]\n",
    "    final_answer = cluster_and_majority_vote(answers, threshold=threshold)\n",
    "    return final_answer, answers\n",
    "\n",
    "run_tests(sampled_dataset,sc_with_clustering)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
