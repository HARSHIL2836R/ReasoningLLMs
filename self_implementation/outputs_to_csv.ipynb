{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90ac20b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harshil/miniconda3/envs/rllms_py39/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "from together import Together\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b01cb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Together client\n",
    "os.environ[\"TOGETHER_API_KEY\"] =\"3116fd3668302432d738187aa87a1ef3d5f89559b73d736b02f31da487baebd0\"\n",
    "client = Together()\n",
    "\n",
    "# Set Together AI API Key\n",
    "# os.environ[\"TOGETHERAI_API_KEY\"] = os.getenv(\"TOGETHER_API_KEY\")\n",
    "\n",
    "# Load a random subset of 3 questions from the validation set\n",
    "dataset = load_dataset(\"commonsense_qa\", split=\"validation\")\n",
    "sampled_dataset = random.sample(list(dataset), 100)\n",
    "\n",
    "def format_prompt(question, choices):\n",
    "    choice_str = \"\\n\".join([f\"{label}. {text}\" for label, text in zip(choices['label'], choices['text'])])\n",
    "    return f\"Question: {question}\\nChoices:\\n{choice_str}\\nAnswer with the correct letter. Give explaination in the next line.\"\n",
    "\n",
    "def get_response(prompt, temperature=0.0):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def self_consistent_answer(question, choices, n_samples=3):\n",
    "    prompt = format_prompt(question, choices)\n",
    "    answers = [get_response(prompt, temperature=0.7) for _ in range(n_samples)]\n",
    "    # print(\"Prompt:\")\n",
    "    # print(prompt)\n",
    "    # print(\"Answers:\")\n",
    "    # for answer in answers:\n",
    "        # print(\"------\")\n",
    "        # print(answer)\n",
    "        # print(\"------\")\n",
    "\n",
    "    return Counter(answers).most_common(1)[0][0],answers\n",
    "\n",
    "# Evaluate self-consistency\n",
    "self_consistency_accuracy = 0\n",
    "greedy_accuracy = 0\n",
    "difference = 0\n",
    "better = 0\n",
    "worse = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fb10630",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['Question', 'Answer Match Greedy', 'Answer Match Self-consistency', 'Correct Answer', 'Greedy Answer', 'Self Answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0293c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, item in enumerate(sampled_dataset, 1):\n",
    "    df.loc[idx, 'Question'] = item[\"question\"]\n",
    "    # print(\"Greedy Decode:\")\n",
    "    greedy_answer = get_response(format_prompt(item[\"question\"], item[\"choices\"]), temperature=0.0)\n",
    "    # print(greedy_answer)\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    n = 4\n",
    "    # print(f\"\\nSelf-Consistency with {n} sample(s):\")\n",
    "    prediction, answers = self_consistent_answer(item[\"question\"], item[\"choices\"], n_samples=n)\n",
    "    # print(f\"Most common answer: {prediction}\")\n",
    "    correct_answer = item[\"answerKey\"]\n",
    "    df.loc[idx, 'Correct Answer'] = correct_answer\n",
    "\n",
    "    if correct_answer == greedy_answer[0]:\n",
    "        df.loc[idx, 'Answer Match Greedy'] = True\n",
    "        greedy_accuracy += 1\n",
    "    else :\n",
    "        df.loc[idx, 'Answer Match Greedy'] = False\n",
    "    \n",
    "    if correct_answer == prediction[0]:\n",
    "        df.loc[idx, 'Answer Match Self-consistency'] = True\n",
    "        self_consistency_accuracy += 1\n",
    "    else :\n",
    "        df.loc[idx, 'Answer Match Self-consistency'] = False\n",
    "    \n",
    "    if greedy_answer.split('\\n')[0][0] != prediction.split('\\n')[0][0]:\n",
    "        print(\"Answers Mismatch\")\n",
    "        print(\"Greedy Decode:\")\n",
    "        print(greedy_answer)\n",
    "        print(\"Self consistency with 3 paths\")\n",
    "        # print(\"Three Answers:\")\n",
    "        # print(answers[0])\n",
    "        # print(answers[1])\n",
    "        # print(answers[2])\n",
    "        print(\"Prediction:\")\n",
    "        print(prediction)\n",
    "        difference += 1\n",
    "        if prediction[0] == correct_answer:\n",
    "            better += 1\n",
    "        if greedy_answer[0] == correct_answer:\n",
    "            worse += 1\n",
    "    else:\n",
    "        print(\"Answers match\")\n",
    "\n",
    "print(\"--- Evaluation ---\")\n",
    "print(\"self_consistency_accuracy \", self_consistency_accuracy, \"%\")\n",
    "print(\"greedy_accuracy \", greedy_accuracy, \"%\")\n",
    "print(\"difference \", difference, \"%\")\n",
    "print(\"better \", better, \"%\")\n",
    "print(\"worse \", worse, '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rllms_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
